{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A11DT-3ba6pI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "KpE_BIyQb1yF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vswwy2bab4ea",
        "outputId": "5d6765ed-817b-4f90-d0a7-4db4aef79f7d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OsZW0q0b6sy",
        "outputId": "01cc15a8-7f6b-4ada-b5db-dc7f4d08a1e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = {i:j for j,i in enumerate(chars)}\n",
        "index_to_char = {i:j for i,j in enumerate(chars)}\n",
        "encode = lambda s:[char_to_index[i] for i in s]\n",
        "decode = lambda s:''.join([index_to_char[i] for i in s])"
      ],
      "metadata": {
        "id": "hVhvub3nb9wz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode('siva sankar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2rTJ1Wdb_wT",
        "outputId": "823fcd92-bce4-420d-d490-3417a1ed5ca0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[57, 47, 60, 39, 1, 57, 39, 52, 49, 39, 56]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode([57, 47, 60, 39, 1, 57, 39, 52, 49, 39, 56])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gTJyseVFcB0Z",
        "outputId": "db3d1a73-2229-4e24-dd29-76d76ddcf501"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'siva sankar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 #training max weight updates\n",
        "eval_interval = 100 #evalution after every \n",
        "learning_rate = 1e-3 #learing rate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #device\n",
        "device = 'cpu'\n",
        "n_embd = 64 # w2v or embedding vector size\n",
        "n_heads = 4 # multihead_attention no.of head\n",
        "blocks = 4  # no.of multihead_attention blocks\n",
        "dropout = 0.0 #drop out\n",
        "head_size = n_embd // n_heads # q,k,v matrix output length"
      ],
      "metadata": {
        "id": "Aa7o24OycMDH"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Initial_embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size,n_embd) #embedding layer\n",
        "\n",
        "    def forward(self,idx):\n",
        "        # here idx (B,T)\n",
        "        x = self.embedding(idx)  # (B,T,n_embd)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pRFefmTMbCfq"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Position_embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.position = nn.Embedding(block_size,n_embd)\n",
        "    def forward(self,idx):\n",
        "        # here  \n",
        "        B,T,C = idx.shape\n",
        "        idx += self.position(torch.arange(T,device=device)) #(B,T,n_embd) + (T,n_emnd) = (B,T,n_embd)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "92YlYpEObLqi"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.key = nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.values = nn.Linear(n_embd,head_size,bias=False)\n",
        "        self.tril = torch.tril(torch.ones(block_size,block_size)).to(device)\n",
        "\n",
        "    def forward(self,idx):\n",
        "\n",
        "        B,T,n_embd = idx.shape\n",
        "        q = self.query(idx)  ## (B,T,n_embd) * (n_embd,head_size) = (B,T,head_size)\n",
        "        k = self.key(idx)    ## (B,T,n_embd) * (n_embd,head_size) = (B,T,head_size)\n",
        "        v = self.values(idx) ## (B,T,n_embd) * (n_embd,head_size) = (B,T,head_size)\n",
        "        x = q @ k.transpose(-2,-1)/head_size**0.5   # (B,T,head_size) * (B,head_size,T) = (B,T,T)\n",
        "        x = x.masked_fill(self.tril[:T,:T]==0,float('-inf'))  \n",
        "        x = F.softmax(x,dim=-1)\n",
        "        x = x @ v            ## (B,T,T) * (B,T,head_size) = (B,T,head_size)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "hPGHcgGZbO9L"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([HeadAttention() for i in range(n_heads)])\n",
        "        self.linear = nn.Linear(n_embd,n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "QOGzE7KTbRaD"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * head_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * head_size, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "JVoEyDkkbWwN"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.normlayer = nn.LayerNorm(n_embd)\n",
        "        self.multihead = MultiHeadAttention()\n",
        "        self.feedforward = FeedFoward()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = self.multihead(x)\n",
        "        x1 += x\n",
        "        x1 = self.normlayer(x1)\n",
        "        x1 = self.feedforward(x1)\n",
        "        x1 += x \n",
        "        x1 = self.normlayer(x1)\n",
        "\n",
        "        return x1"
      ],
      "metadata": {
        "id": "-bKwmoZHbZkW"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = Initial_embedding()\n",
        "        self.position_embedding = Position_embedding()\n",
        "        self.blocks = nn.Sequential(*[Block() for i in range(blocks)])\n",
        "        self.linear = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "    def forward(self,x,targets=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.position_embedding(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = x.shape\n",
        "            x = x.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(x, targets)\n",
        "\n",
        "        return x,loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "RH3HEZDubdWH"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train test split"
      ],
      "metadata": {
        "id": "7IXRhbOieBh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text))\n",
        "print(data.shape)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKDF_kmPeLFQ",
        "outputId": "9b59bf6b-2b5c-4eba-cc25-10cb9b661733"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upto = int(len(data)*90/100)\n",
        "train_data = data[:upto]\n",
        "valid_data = data[upto:]\n",
        "train_data.shape,valid_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc8bLVWneAz2",
        "outputId": "10823a41-bade-4f6b-a2d1-98f49eb45ff1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1003854]), torch.Size([111540]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100) # seed\n",
        "\n",
        "def get_batch(data):\n",
        "    data = train_data if data=='train' else valid_data\n",
        "    start_points = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[start:start+block_size] for start in start_points])\n",
        "    y = torch.stack([data[start+1:start+block_size+1] for start in start_points])\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "Mm3vsYUpd5mN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT()\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "sjQ_dQ7rpwJu"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if iter % eval_interval == 0:\n",
        "        xb,yb = get_batch('valid')\n",
        "        logits,valid_loss = model(xb,yb)\n",
        "        print(f\"iteration {iter} train_loss {loss} valid_loss {valid_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOajpTuKpx-i",
        "outputId": "a108bbb4-ba8d-4fad-f765-1aadb0070b73"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 train_loss 1.6059319972991943 valid_loss 1.8289523124694824\n",
            "iteration 100 train_loss 1.7760330438613892 valid_loss 1.9158116579055786\n",
            "iteration 200 train_loss 1.771547794342041 valid_loss 2.128838539123535\n",
            "iteration 300 train_loss 1.8115777969360352 valid_loss 1.75994074344635\n",
            "iteration 400 train_loss 1.8020707368850708 valid_loss 1.7162811756134033\n",
            "iteration 500 train_loss 1.8282545804977417 valid_loss 1.9052622318267822\n",
            "iteration 600 train_loss 1.7830768823623657 valid_loss 1.8486415147781372\n",
            "iteration 700 train_loss 1.680596113204956 valid_loss 1.850701928138733\n",
            "iteration 800 train_loss 1.742267370223999 valid_loss 1.9102754592895508\n",
            "iteration 900 train_loss 1.7285118103027344 valid_loss 1.9047157764434814\n",
            "iteration 1000 train_loss 1.6784862279891968 valid_loss 2.0332984924316406\n",
            "iteration 1100 train_loss 1.673144817352295 valid_loss 1.754084825515747\n",
            "iteration 1200 train_loss 1.6554232835769653 valid_loss 1.8107984066009521\n",
            "iteration 1300 train_loss 1.7195836305618286 valid_loss 1.787856936454773\n",
            "iteration 1400 train_loss 1.779296636581421 valid_loss 1.8354437351226807\n",
            "iteration 1500 train_loss 1.6367273330688477 valid_loss 1.8069257736206055\n",
            "iteration 1600 train_loss 1.768911361694336 valid_loss 1.8342313766479492\n",
            "iteration 1700 train_loss 1.695396900177002 valid_loss 1.881750226020813\n",
            "iteration 1800 train_loss 1.7203437089920044 valid_loss 1.782575011253357\n",
            "iteration 1900 train_loss 1.6378192901611328 valid_loss 1.897560954093933\n",
            "iteration 2000 train_loss 1.5522953271865845 valid_loss 1.9343459606170654\n",
            "iteration 2100 train_loss 1.6404649019241333 valid_loss 1.768727421760559\n",
            "iteration 2200 train_loss 1.6181703805923462 valid_loss 1.8712468147277832\n",
            "iteration 2300 train_loss 1.5719075202941895 valid_loss 2.0725536346435547\n",
            "iteration 2400 train_loss 1.8262059688568115 valid_loss 1.7943776845932007\n",
            "iteration 2500 train_loss 1.6904839277267456 valid_loss 1.8185639381408691\n",
            "iteration 2600 train_loss 1.693455696105957 valid_loss 1.8908069133758545\n",
            "iteration 2700 train_loss 1.6470632553100586 valid_loss 1.8845945596694946\n",
            "iteration 2800 train_loss 1.6733167171478271 valid_loss 1.8092464208602905\n",
            "iteration 2900 train_loss 1.917630672454834 valid_loss 1.8215967416763306\n",
            "iteration 3000 train_loss 1.5767953395843506 valid_loss 1.7268537282943726\n",
            "iteration 3100 train_loss 1.6743627786636353 valid_loss 1.7514781951904297\n",
            "iteration 3200 train_loss 1.5965795516967773 valid_loss 1.8416889905929565\n",
            "iteration 3300 train_loss 1.7469472885131836 valid_loss 1.7690517902374268\n",
            "iteration 3400 train_loss 1.7460541725158691 valid_loss 1.7554935216903687\n",
            "iteration 3500 train_loss 1.7813595533370972 valid_loss 1.7693126201629639\n",
            "iteration 3600 train_loss 1.7995022535324097 valid_loss 1.764634609222412\n",
            "iteration 3700 train_loss 1.6460202932357788 valid_loss 1.7338913679122925\n",
            "iteration 3800 train_loss 1.6199305057525635 valid_loss 1.7211031913757324\n",
            "iteration 3900 train_loss 1.6451714038848877 valid_loss 1.8886239528656006\n",
            "iteration 4000 train_loss 1.6932671070098877 valid_loss 1.8008878231048584\n",
            "iteration 4100 train_loss 1.6803905963897705 valid_loss 1.8279818296432495\n",
            "iteration 4200 train_loss 1.771864652633667 valid_loss 1.8166924715042114\n",
            "iteration 4300 train_loss 1.6655094623565674 valid_loss 1.8672971725463867\n",
            "iteration 4400 train_loss 1.6788207292556763 valid_loss 1.848605990409851\n",
            "iteration 4500 train_loss 1.6294262409210205 valid_loss 1.8935071229934692\n",
            "iteration 4600 train_loss 1.6799371242523193 valid_loss 1.7119231224060059\n",
            "iteration 4700 train_loss 1.550460696220398 valid_loss 1.7912300825119019\n",
            "iteration 4800 train_loss 1.6712391376495361 valid_loss 1.8998266458511353\n",
            "iteration 4900 train_loss 1.6489146947860718 valid_loss 1.7608580589294434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'hello'\n",
        "text = encode(text)\n",
        "print(decode(model.generate(torch.tensor([text]),max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri4W_L_cq5Iv",
        "outputId": "6d39d594-3644-4fda-b48f-533d49aadfc2"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hellow see alouder beneing'd,\n",
            "Not is marrowed noble more so shand all:\n",
            "On he'lls to and the peopt-hacine contacinen'd, I wicks\n",
            "Had call very massure to pary him. Would pribled by, thought, whence Genessure armone of youll I begge to from tight,\n",
            "Before it doth\n",
            "To the beater; hone, my lord, all thy wring sofder\n",
            "Wliccent, was out humes; whicher since.\n",
            "\n",
            "CLAs:\n",
            "What'derneral it you ending!\n",
            "\n",
            "DUCHESS:\n",
            "Erely haven when I'ld throught he burthers,\n",
            "You stroady, my her, again it inspulcient to die.\n",
            "\n",
            "ERWIETEN ELIZA:\n",
            "Farder only  will men may they be trudge,\n",
            "On Mieck; why then, that consul,\n",
            "Where are lawly the life bloody.\n",
            "\n",
            "LEONTES:\n",
            "O, with there offor, then at himself, I pray\n",
            "Rever, reyet with daughters to for can me,\n",
            "And ibsuing, then the's auld towere, then,\n",
            "Thir rewards wivis fleom my it,\n",
            "good none will words be Edward well, your shame,\n",
            "In hoot provent doth the days\n",
            "Of whas you think husband of lord,\n",
            "If that the say thank their equling of a corcend the rougness,\n",
            "Grater would it be goody to with amot t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GmtZHi1crG1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}